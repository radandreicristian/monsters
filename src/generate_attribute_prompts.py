import asyncio
import json

import typer

from dotenv import dotenv_values
import os
from src.llm.openai import OpenAiLlmClient
from src.utils.prompts import add_attributes_to_template_prompt
from src.utils.mapping import id_to_local_name
from src.utils.async_wrapper import run_async
from src.logger.utils import get_logger

logger = get_logger()

config = {
    **dotenv_values("src/config/.env"),
    **dotenv_values("src/config/.secret.env"),
    **os.environ
}


@run_async
async def main(model_id: str,
               templates_path: str = 'data/templates.json') -> None:
    """
    Compute generation prompts using the templates in templates_path, with the attributes extracted from the images generated by a model.

    model_id: The (internal) id of the model, e.g. 'sd3'. See src/tti/__init__.py
    templates_path: The location of the image generation templates.
    """
    llm = OpenAiLlmClient(**config)

    with open(templates_path, "r") as f:
        templates = json.load(f)
    
    local_model_name = id_to_local_name[model_id]

    model_attributes_path = f"data/attributes/{local_model_name}"
    
    for attribute in os.listdir(model_attributes_path):
        for formulation in ("direct", "indirect"):
            attributes_path = os.path.join(model_attributes_path, attribute, formulation, "attributes.json")
            with open(attributes_path, "r") as f:
                attributes_dict = json.load(f)
                attributes = ", ".join([f"{k}: {v}" for k, v in attributes_dict.items()])
            tasks = []

            formulation_templates = templates[formulation]
            for template in formulation_templates:
                prompt = add_attributes_to_template_prompt.format(template=template,
                                                                    attributes=attributes)
                logger.info(prompt)
                tasks.append(llm.reply(prompt))
            responses = await asyncio.gather(*tasks)

            # make sure we're lower capping everything to be consistent with the rest
            responses = [r.lower() for r in responses]

            # output like data/biased_prompts/model/attribute/formulation/biased_generation_prompts.json
            path_dir = os.path.join("data/biased_prompts", local_model_name, attribute, formulation)

            os.makedirs(path_dir, exist_ok=True)

            with open(os.path.join(path_dir, 'biased_generation_prompts.json'), 'w') as f:
                json.dump(responses, f, indent=2)


if __name__ == '__main__':
    typer.run(main)
